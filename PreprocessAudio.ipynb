{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessAudio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtSTmDnBYoLzwhZy9AAGMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kregier/AudioLanguageClassifer/blob/main/PreprocessAudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy5nbC1AJWce",
        "outputId": "db6ccf02-12ac-40b8-b492-0654cc3ee2c6"
      },
      "source": [
        "# Set up the environment\n",
        "!pip install soundfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"All set up!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "All set up!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyCNEILJcdp",
        "outputId": "00a9dd07-ad7f-43d5-b81b-e5582f0a34ca"
      },
      "source": [
        "# Set up the data import using Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wJpvvK3JfWt",
        "outputId": "75f06557-791f-4708-ec17-7a40e0ab0767"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "\n",
        "# Change working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n",
            "data  kaggle.json  reading-passage.txt\trecordings  speakers_all.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "AvcxLAB7JvVK",
        "outputId": "7867214c-7dd5-456a-d202-0326831a6942"
      },
      "source": [
        "meta = pd.read_csv('speakers_all.csv')\n",
        "# Prepare the data based on previous exploration\n",
        "# Drop 3 end columns with NaN values\n",
        "meta.drop(['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'], axis=1, inplace=True)\n",
        "\n",
        "# Set speakerid as index\n",
        "meta.set_index('speakerid', inplace=True)\n",
        "meta.sort_index(inplace=True)\n",
        "\n",
        "# Replace missing values and typos\n",
        "meta.loc[meta.country.isnull(), 'country'] = 'laos'\n",
        "type_idx = meta[meta.sex =='famale'].index\n",
        "meta.loc[type_idx, 'sex'] = 'female'\n",
        "\n",
        "# Delete records with missing audio files\n",
        "missingIdx = meta[meta['file_missing?']==True].index\n",
        "meta.drop(missingIdx, inplace=True )\n",
        "\n",
        "# Delete records with no birthplace - synthesized files\n",
        "meta.dropna(subset=['birthplace'], inplace=True)\n",
        "\n",
        "# Delete files not present in audiofiles database\n",
        "nica_index = meta[meta.filename == 'nicaragua'].index\n",
        "sinhalese_index = meta[meta.filename=='sinhalese1'].index\n",
        "meta.drop(nica_index, inplace=True, axis=0)\n",
        "meta.drop(sinhalese_index, inplace=True, axis=0)\n",
        "\n",
        "meta.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>age_onset</th>\n",
              "      <th>birthplace</th>\n",
              "      <th>filename</th>\n",
              "      <th>native_language</th>\n",
              "      <th>sex</th>\n",
              "      <th>country</th>\n",
              "      <th>file_missing?</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speakerid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>virginia, south africa</td>\n",
              "      <td>afrikaans1</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>female</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>pretoria, south africa</td>\n",
              "      <td>afrikaans2</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>diekabo, ivory coast</td>\n",
              "      <td>agni1</td>\n",
              "      <td>agni</td>\n",
              "      <td>male</td>\n",
              "      <td>ivory coast</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>prishtina, kosovo</td>\n",
              "      <td>albanian1</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>kosovo</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>tirana, albania</td>\n",
              "      <td>albanian2</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>albania</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            age  age_onset  ...       country file_missing?\n",
              "speakerid                   ...                            \n",
              "1          27.0        9.0  ...  south africa         False\n",
              "2          40.0        5.0  ...  south africa         False\n",
              "3          25.0       15.0  ...   ivory coast         False\n",
              "4          19.0        6.0  ...        kosovo         False\n",
              "5          33.0       15.0  ...       albania         False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZk66l_ZOP5a"
      },
      "source": [
        "# Set constants\n",
        "SAMP_RATE = 16000\n",
        "BATCH_SIZE = 6"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxfdXrnVKxCY",
        "outputId": "25ede05d-2ef0-40af-86fb-aa7a9c1c0b8f"
      },
      "source": [
        "# Split data into training and testing sets for gender analysis\n",
        "data = meta[['sex', 'filename']]\n",
        "x_train_names, x_test_names, y_train, y_test = train_test_split(data['filename'], \n",
        "                                                                data['sex'], \n",
        "                                                                test_size=0.25, \n",
        "                                                                random_state=38, \n",
        "                                                                stratify=data['sex'])\n",
        "print(x_train_names.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J2r1RxRKyur",
        "outputId": "ecd0283a-09a5-4e19-95f3-e315ab1638b1"
      },
      "source": [
        "print(x_test_names.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(534,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQeI97MG5x0"
      },
      "source": [
        "Since I can't hold all of the sound files and their augmentations (segmetation, noise addition and VGGish embedding) in memory, I going to write each segement and noise to file, so they can be loaded one by one. I can't figure out how to make this work with tf.Data, since all of the examples use data that is available through TF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUs-hu5YLA5G"
      },
      "source": [
        "# Scale audio to fall between [-1, 1]\n",
        "def normalize(audio):\n",
        "  norm = audio/max(audio)\n",
        "  return norm"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3GVMXSzKV9M"
      },
      "source": [
        "def segment_10s(audio, sr):\n",
        "  \"\"\" Load an audio file and divide into 10 second segments.\n",
        "  Arguments: audio - the audio file; sr = sampling rate of the file\n",
        "  Returns: a dictionary of the audio segments. Key is the index of segment, value is the segment.\n",
        "  \"\"\"\n",
        "  seg_files ={}\n",
        "  n_seg = int((len(audio)/sr)/10)\n",
        "  for i in range(n_seg):\n",
        "    segment = audio[10*i*sr:(i+1)*10*sr]\n",
        "    seg_files[i] = segment\n",
        "  return seg_files"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmX_m9xHYRT"
      },
      "source": [
        "1. Read each audio file in x_train or x_test\n",
        "2. Segment the audio file into 10s segments\n",
        "3. Save each segment to file by appending segment index to filename."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PhrD6U0IMa0"
      },
      "source": [
        "def segment_data(x_names, y_names, split='train', clf='gender'):\n",
        "  seg_names = []\n",
        "\n",
        "  for i in range(len(x_names)): #df = x_train_names, x_test_names\n",
        "    filename = x_names.iloc[i]\n",
        "    filepath = 'recordings/recordings/' + filename + '.mp3'\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        "    audio = normalize(audio)\n",
        "\n",
        "    # Add gender label to filename, for later processing\n",
        "    sex = y_names.iloc[i]\n",
        "    if sex == \"female\":\n",
        "      filename = '{}.F'.format(filename)\n",
        "    else: filename = '{}.M'.format(filename)\n",
        "\n",
        "    # Segment audio file\n",
        "    seg_files = segment_10s(audio, SAMP_RATE)\n",
        "    for key, val in seg_files.items():\n",
        "      new_name = '{}.{}'.format(filename, key)\n",
        "      sf.write('data/{}/{}/{}o.wav'.format(clf, split, new_name), val, SAMP_RATE)\n",
        "      seg_names.append(new_name)\n",
        "    # end filename\n",
        "  return seg_names"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZxCJJjERReK",
        "outputId": "87ec520c-ca5a-473d-b15d-dc0c285834f7"
      },
      "source": [
        "x_train_seg = segment_data(x_train_names[:10], y_train, split='train', clf='gender')\n",
        "print(len(x_train_seg))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8TMxuNXRrmw",
        "outputId": "1b8bcba3-f0ff-4d9a-8399-00058318784b"
      },
      "source": [
        "x_test_seg = segment_data(x_test_names[:10], y_test, split='test', clf='gender')\n",
        "print(len(x_test_seg))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJxRO0lLIgRm"
      },
      "source": [
        "Add noise\n",
        "1. Read each audio file (10s segments)\n",
        "2. Add random noise\n",
        "3. Save noisy segement to file by appending noise to filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCT-10IdIuJW"
      },
      "source": [
        "def add_noise(audio):\n",
        "    '''\n",
        "    Add random noise to an audio file.\n",
        "    Arguments: audio - the audio file\n",
        "    Returns: the noisy audio file\n",
        "    ''' \n",
        "    # Load random number generator\n",
        "    rng = np.random.default_rng()\n",
        "    # Generate random noise\n",
        "    noise = rng.standard_normal(len(audio))\n",
        "    # Add noise to file\n",
        "    noisy_seg = audio + 0.005*noise\n",
        "\n",
        "    return noisy_seg"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdSiGpinR1Qe"
      },
      "source": [
        "def noisy_data(x_names, split='train', clf='gender'):\n",
        "  for i in range(len(x_names)): #list of seg_names\n",
        "    filename = x_names[i]\n",
        "    filepath = 'data/{}/{}/{}o.wav'.format(clf, split, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        " #   audio = normalize(audio) #Already done when originally segmented\n",
        "    # Add noise\n",
        "    noisy = add_noise(audio)\n",
        "    # Write noise to file\n",
        "    sf.write('data/{}/{}/{}n.wav'.format(clf, split, filename), noisy, SAMP_RATE)\n",
        "    print(\"Noise added to {}\".format(x_names[i]))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcH0tkBDkNHP",
        "outputId": "29e6039b-d486-4197-f10d-94c415941372"
      },
      "source": [
        "# Generate noisy samples\n",
        "noisy_data(x_train_seg[:15], split='train', clf='gender')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noise added to english188.M.0\n",
            "Noise added to english188.M.1\n",
            "Noise added to english413.F.0\n",
            "Noise added to english413.F.1\n",
            "Noise added to italian28.M.0\n",
            "Noise added to italian28.M.1\n",
            "Noise added to xiang3.M.0\n",
            "Noise added to xiang3.M.1\n",
            "Noise added to xiang3.M.2\n",
            "Noise added to english529.F.0\n",
            "Noise added to french54.F.0\n",
            "Noise added to french54.F.1\n",
            "Noise added to english263.M.0\n",
            "Noise added to english263.M.1\n",
            "Noise added to swedish1.F.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7TaCs4WkfVC"
      },
      "source": [
        "# No need to add noise to test files!\n",
        "#noisy_data(x_test_seg[:10], split='test', clf='gender')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WogZrOwRkhRz",
        "outputId": "50de379b-13ff-4976-963d-56f54dd6a6b8"
      },
      "source": [
        "!ls data/gender/train/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english188.M.0n.wav  english413.F.0n.wav  french54.F.1o.wav   swedish1.F.1o.wav\n",
            "english188.M.0o.wav  english413.F.0o.wav  italian28.M.0n.wav  xiang3.M.0n.wav\n",
            "english188.M.1n.wav  english413.F.1n.wav  italian28.M.0o.wav  xiang3.M.0o.wav\n",
            "english188.M.1o.wav  english413.F.1o.wav  italian28.M.1n.wav  xiang3.M.1n.wav\n",
            "english263.M.0n.wav  english529.F.0n.wav  italian28.M.1o.wav  xiang3.M.1o.wav\n",
            "english263.M.0o.wav  english529.F.0o.wav  serbian7.F.0o.wav   xiang3.M.2n.wav\n",
            "english263.M.1n.wav  french54.F.0n.wav\t  serbian7.F.1o.wav   xiang3.M.2o.wav\n",
            "english263.M.1o.wav  french54.F.0o.wav\t  swedish1.F.0n.wav\n",
            "english272.M.0o.wav  french54.F.1n.wav\t  swedish1.F.0o.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1FjJOw0kqG6",
        "outputId": "2bcf8c84-409b-4830-adaa-421fe0ec1988"
      },
      "source": [
        "!ls data/gender/test/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arabic48.M.0o.wav    english579.M.1o.wav  romanian5.F.1o.wav\n",
            "arabic48.M.1o.wav    estonian1.M.0o.wav   vietnamese15.M.0o.wav\n",
            "arabic48.M.2o.wav    estonian1.M.1o.wav   vietnamese15.M.1o.wav\n",
            "english122.F.0o.wav  filipino1.M.0o.wav   vietnamese15.M.2o.wav\n",
            "english122.F.1o.wav  japanese5.F.0o.wav   vietnamese15.M.3o.wav\n",
            "english143.M.0o.wav  japanese5.F.1o.wav   wolof6.M.0o.wav\n",
            "english143.M.1o.wav  japanese5.F.2o.wav   wolof6.M.1o.wav\n",
            "english579.M.0o.wav  romanian5.F.0o.wav   wolof6.M.2o.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOATRYRKIc5Q"
      },
      "source": [
        "Convert to VGGish embedding\n",
        "1. Load VGGish model\n",
        "1. Read each audio file (10s segments with and without noise)\n",
        "1. Run through VGGish model\n",
        "1. Save embedding by appending _embed to filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqw7s0amUV-U"
      },
      "source": [
        "# Using a SavedModel from the TFHub in Keras\n",
        "# https://www.tensorflow.org/hub/tf2_saved_model\n",
        "# VGGish model, from https://tfhub.dev/google/vggish/1\n",
        "\n",
        "# Link to the model on TFHub\n",
        "hub_url = 'https://tfhub.dev/google/vggish/1'\n",
        "\n",
        "# Load the model as a Keras model\n",
        "vggish_model = hub.KerasLayer(hub_url)\n",
        "vggish_model.trainable = False\n",
        "\n",
        "# Load the model as a tf.function - not needed, since it doesn't work\n",
        "#vggish_fn = hub.load(hub_url)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBJeBKHoUdc7",
        "outputId": "65e8c1c1-6825-479b-8a4f-ae0898887e7b"
      },
      "source": [
        "x_train_filenames = os.listdir('./data/gender/train')\n",
        "print(x_train_filenames)\n",
        "\n",
        "x_train_filepaths = ['./data/gender/train/{}'.format(i) for i in x_train_filenames]\n",
        "print(len(x_train_filepaths))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['english188.M.0o.wav', 'english188.M.1o.wav', 'english413.F.0o.wav', 'english413.F.1o.wav', 'italian28.M.0o.wav', 'italian28.M.1o.wav', 'xiang3.M.0o.wav', 'xiang3.M.1o.wav', 'xiang3.M.2o.wav', 'english529.F.0o.wav', 'french54.F.0o.wav', 'french54.F.1o.wav', 'english263.M.0o.wav', 'english263.M.1o.wav', 'swedish1.F.0o.wav', 'swedish1.F.1o.wav', 'english272.M.0o.wav', 'serbian7.F.0o.wav', 'serbian7.F.1o.wav', 'english188.M.0n.wav', 'english188.M.1n.wav', 'english413.F.0n.wav', 'english413.F.1n.wav', 'italian28.M.0n.wav', 'italian28.M.1n.wav', 'xiang3.M.0n.wav', 'xiang3.M.1n.wav', 'xiang3.M.2n.wav', 'english529.F.0n.wav', 'french54.F.0n.wav', 'french54.F.1n.wav', 'english263.M.0n.wav', 'english263.M.1n.wav', 'swedish1.F.0n.wav']\n",
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYAF9vcyAvk",
        "outputId": "75301955-69c2-44d6-c50b-2c487f4bde89"
      },
      "source": [
        "x_test_filenames = os.listdir('./data/gender/test')\n",
        "print(x_test_filenames)\n",
        "\n",
        "x_test_filepaths = ['./data/gender/test/{}'.format(i) for i in x_test_filenames]\n",
        "print(len(x_train_filepaths))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wolof6.M.0o.wav', 'wolof6.M.1o.wav', 'wolof6.M.2o.wav', 'estonian1.M.0o.wav', 'estonian1.M.1o.wav', 'romanian5.F.0o.wav', 'romanian5.F.1o.wav', 'english122.F.0o.wav', 'english122.F.1o.wav', 'vietnamese15.M.0o.wav', 'vietnamese15.M.1o.wav', 'vietnamese15.M.2o.wav', 'vietnamese15.M.3o.wav', 'arabic48.M.0o.wav', 'arabic48.M.1o.wav', 'arabic48.M.2o.wav', 'english579.M.0o.wav', 'english579.M.1o.wav', 'english143.M.0o.wav', 'english143.M.1o.wav', 'filipino1.M.0o.wav', 'japanese5.F.0o.wav', 'japanese5.F.1o.wav', 'japanese5.F.2o.wav']\n",
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpFjpxAX3YMU"
      },
      "source": [
        "# Define transformation function and dataset generator\n",
        "\n",
        "Adapted from https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-WwlXbp3j_0"
      },
      "source": [
        "def vggish_transform(audio):\n",
        "  return vggish_fn(audio)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVfe-g1PW4Rt"
      },
      "source": [
        "# Write a generator to read data in chunks and process it\n",
        "# Generator yields both data and labels\n",
        "# Takes a list of filenames as first argument, batch_size as second argument\n",
        "\n",
        "#https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/\n",
        "\n",
        "def tf_data_generator(file_list, batch_size=32):\n",
        "  i = 0\n",
        "  while True: #infinite loop\n",
        "    if i*batch_size >= len(file_list):\n",
        "      i=0\n",
        "      np.random.shuffle(file_list)\n",
        "    else:\n",
        "      file_chunk = file_list[i*batch_size:(i+1)*batch_size]\n",
        "      data = []\n",
        "      labels = []\n",
        "      label_classes = tf.constant(['M', 'F'])\n",
        "      for file in file_chunk:\n",
        "        # Read data\n",
        "        audio, sr = librosa.load(file, sr=16000)\n",
        "        # Apply transformations\n",
        "        embed = vggish_model(audio)\n",
        "        data.append(embed)\n",
        "        #data.append(audio)\n",
        "        # Extract labels from filename\n",
        "        bytes_string = file\n",
        "        string_name = str(bytes_string, 'utf-8')\n",
        "        split_str = string_name.split('.')\n",
        "        pattern = tf.constant(split_str[2])\n",
        "        for j in range(len(label_classes)):\n",
        "          if re.match(pattern.numpy(), label_classes[j].numpy()):\n",
        "            labels.append(j)\n",
        "\n",
        "      data = np.asarray(data)\n",
        "      labels = np.asarray(labels)\n",
        "\n",
        "      # To be able t prefecth the data you can use the mpa function, \n",
        "      # but this doesn't work for VGGish, sincc VGGish only processes one file at a time\n",
        "      #first_dim = data.shape[0]\n",
        "      ## Create tensorflow dataset to use 'map' function for parallelization\n",
        "      #data_ds = tf.data.Dataset.from_tensor_slices(data)\n",
        "      #data_ds = data_ds.batch(batch_size = first_dim).map(vggish_transform,\n",
        "                                                         # num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "      # Convert dataset to generator and subsequently to np array\n",
        "      #data_ds = tfds.as_numpy(data_ds)\n",
        "      #data = np.array([data for data in data_ds]).reshape(first_dim, 10, 128)\n",
        "\n",
        "      yield data, labels\n",
        "      i += 1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhzcoHwjjHsr"
      },
      "source": [
        "dataset_check = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_train_filepaths[:12], BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQmW_OYYjkG9",
        "outputId": "1273331d-f85a-4f72-b4d4-2f22ccc608cc"
      },
      "source": [
        "for data, labels in dataset_check.take(2):\n",
        "  print(data.shape)\n",
        "  print(labels)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 10, 128)\n",
            "tf.Tensor([0. 0. 1. 1. 0. 0.], shape=(6,), dtype=float32)\n",
            "(6, 10, 128)\n",
            "tf.Tensor([0. 0. 0. 1. 1. 1.], shape=(6,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEXOQAav2kB"
      },
      "source": [
        "# Create pipeline and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Pk8AuYv1G4"
      },
      "source": [
        "x_train, x_val = train_test_split(x_train_filepaths, test_size=.25, random_state=38)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2zXm1KIw1Jq",
        "outputId": "c4381549-6a49-4065-b0c7-feee68afa07a"
      },
      "source": [
        "# Print sizes of data splits\n",
        "print(\"Number of training samples: \", len(x_train))\n",
        "print(\"Number of training samples: \", len(x_val))\n",
        "print(\"Number of training samples: \", len(x_test_seg))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  25\n",
            "Number of training samples:  9\n",
            "Number of training samples:  24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuO150FTyPnp",
        "outputId": "f3e679e7-767d-41c0-9adf-32602f306567"
      },
      "source": [
        "print(BATCH_SIZE)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVn4nKKoxkrf"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_train, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) ) \n",
        "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_val, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) )\n",
        "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_test_filepaths, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) ) "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGIx7TvhFtO6",
        "outputId": "4bf11360-31a2-4c03-9bc6-e63f2f48f969"
      },
      "source": [
        "# Check structure of datasets, with the goal of extracting the labels\n",
        "\n",
        "# Look at each type of element component\n",
        "test_dataset.element_spec"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kryXvX26e9d"
      },
      "source": [
        "#I don't think this will work, since I can't get VGGish to fuction on multiple files at a time\n",
        "# Prefetch datasets = prepare next batch with CPU while GPU trains on previous batch\n",
        "#train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "#validation_dataset = validation_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxkrPsFeyqoG"
      },
      "source": [
        "# Build and compile the model\n",
        "Copy from other notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNVZkSI8F0_"
      },
      "source": [
        "genderClf = tf.keras.models.Sequential([tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "                              tf.keras.layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\", data_format=\"channels_last\")\n",
        "                              ])\n",
        "genderClf.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIcjLruK8H_V"
      },
      "source": [
        "# Add early stopping to train classifier model; default is 10 epochs\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping_monitor = EarlyStopping(patience=2)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1xgA1Vjyyku"
      },
      "source": [
        "*Important* before fitting model, specify number of epochs and steps to fit, since generators are infinite loops.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAoyS3n1zmW0",
        "outputId": "461f0b31-68cd-497e-a76f-f9495732ab49"
      },
      "source": [
        "# Calculate how many dataset batches to generate, since generator is infinite\n",
        "steps_per_epoch = np.int(np.ceil(len(x_train)/BATCH_SIZE))\n",
        "val_steps = np.int(np.ceil(len(x_val)/BATCH_SIZE))\n",
        "eval_steps = np.int(np.ceil(len(x_test_filepaths)/BATCH_SIZE))\n",
        "\n",
        "print(\"steps_per_epoch = \", steps_per_epoch)\n",
        "print(\"validation_steps = \", val_steps)\n",
        "print(\"steps = \", eval_steps)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "steps_per_epoch =  5\n",
            "validation_steps =  2\n",
            "steps =  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIW93ghK0oS5",
        "outputId": "2d397129-48da-4ef7-bbfa-87c5f0f1bea4"
      },
      "source": [
        "#model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
        "#         validation_steps = val_steps, epochs = 5)\n",
        "\n",
        "history = genderClf.fit(train_dataset,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        epochs=20,\n",
        "                        validation_data=validation_dataset,\n",
        "                        validation_steps = val_steps,\n",
        "                        callbacks=[early_stopping_monitor], \n",
        "                        batch_size=BATCH_SIZE)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.6598 - accuracy: 0.5200 - val_loss: 0.5575 - val_accuracy: 0.6667\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 18s 4s/step - loss: 0.4332 - accuracy: 0.9200 - val_loss: 0.3616 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.3058 - accuracy: 1.0000 - val_loss: 0.2583 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 18s 4s/step - loss: 0.2052 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 18s 4s/step - loss: 0.1418 - accuracy: 1.0000 - val_loss: 0.1366 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JymMio95636G",
        "outputId": "c7a0cb52-3d42-4fa1-f163-8ae2f1a35638"
      },
      "source": [
        "# Evaluate model?\n",
        "test_loss, test_acc = genderClf.evaluate(test_dataset, steps=eval_steps)\n",
        "#print(test_loss)\n",
        "#print(test_acc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 12s 3s/step - loss: 0.0987 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izdCh1mt7cMI"
      },
      "source": [
        "# Making predictions\n",
        "y_pred = genderClf.predict(test_dataset, steps=eval_steps)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX4ouOufFf3_",
        "outputId": "ccaa7f5e-9919-4e9d-dbda-30c2ccb4f569"
      },
      "source": [
        "y_test_labels = []\n",
        "for data, labels in test_dataset.take(eval_steps):\n",
        "  y_test_labels.append(labels.numpy())\n",
        "\n",
        "print(type(y_test_labels))\n",
        "print(len(y_test_labels))\n",
        "print(y_test_labels)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "4\n",
            "[array([0., 0., 0., 0., 0., 1.], dtype=float32), array([1., 1., 1., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 1., 1., 1.], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn5YwvTHK_42",
        "outputId": "6ebe1112-a88d-453c-c63f-e86413b8cbc3"
      },
      "source": [
        "print(y_pred.shape)\n",
        "y_pred = y_pred [:, 0, 0]\n",
        "print(y_pred.shape)\n",
        "#print(y_pred)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24, 1, 1)\n",
            "(24,)\n",
            "[0.00576037 0.01786832 0.00544906 0.00657452 0.05765942 0.99489546\n",
            " 0.98236066 0.9427161  0.79921967 0.10303779 0.01304106 0.3022809\n",
            " 0.0112198  0.28809893 0.32841498 0.4606599  0.00174568 0.00206399\n",
            " 0.00133762 0.00421037 0.05847541 0.99983454 0.9517562  0.99930394]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmmI4-F6LZZ_",
        "outputId": "97565317-877a-4286-bfd6-8851838377b3"
      },
      "source": [
        "y_pred_int  = []\n",
        "for i in y_pred:\n",
        "  if i < 0.5:\n",
        "    y_pred_int.append(0)\n",
        "  else: y_pred_int.append(1)\n",
        "\n",
        "print(y_pred_int)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFgg_CWBYkg"
      },
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "To create a dataset of all files matching a pattern, use tf.data.Dataset.list_files:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOxdxJM4BVX4"
      },
      "source": [
        "#dataset = tf.data.Dataset.list_files(\"/path/*.txt\")  # doctest: +SKIP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106Hn5J_CX2G"
      },
      "source": [
        "batch\n",
        "View source\n",
        "\n",
        "batch(\n",
        "    batch_size, drop_remainder=False\n",
        ")\n",
        "\n",
        "Combines consecutive elements of this dataset into batches.\n",
        "\n",
        "dataset = tf.data.Dataset.range(8)\n",
        "dataset = dataset.batch(3)\n",
        "list(dataset.as_numpy_iterator())\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.range(8)\n",
        "dataset = dataset.batch(3, drop_remainder=True)\n",
        "list(dataset.as_numpy_iterator())\n",
        "\n",
        "\n",
        "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DSYfePD-zV"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/audio/simple_audio\n",
        "\n",
        "Simple audio recognition: Recognizing keywords"
      ]
    }
  ]
}