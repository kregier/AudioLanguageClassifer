{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessAudio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXo7iVmL27A4zQxRV0qe4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kregier/AudioLanguageClassifer/blob/main/PreprocessAudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy5nbC1AJWce",
        "outputId": "194cf09e-7b10-4c8a-fcf3-1000737e6317"
      },
      "source": [
        "# Set up the environment\n",
        "!pip install soundfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "#import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"All set up!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n",
            "All set up!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyCNEILJcdp",
        "outputId": "f68d3608-2c3a-4ac3-ce35-4c7606956b68"
      },
      "source": [
        "# Set up the data import using Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wJpvvK3JfWt",
        "outputId": "8ca35564-c37c-4621-90a2-a69e669fb9d7"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "\n",
        "# Change working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n",
            "augment  kaggle.json\treading-passage.txt  speakers_all.csv\n",
            "data\t processed.csv\trecordings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZk66l_ZOP5a"
      },
      "source": [
        "# Set constants\n",
        "SAMP_RATE = 16000\n",
        "BATCH_SIZE = 6"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "AvcxLAB7JvVK",
        "outputId": "cc9dc987-ae4d-4353-b9a9-a4fc5eca27cd"
      },
      "source": [
        "meta = pd.read_csv('speakers_all.csv')\n",
        "# Prepare the data based on previous exploration\n",
        "# Drop 3 end columns with NaN values\n",
        "meta.drop(['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'], axis=1, inplace=True)\n",
        "\n",
        "# Set speakerid as index\n",
        "meta.set_index('speakerid', inplace=True)\n",
        "meta.sort_index(inplace=True)\n",
        "\n",
        "# Replace missing values and typos\n",
        "meta.loc[meta.country.isnull(), 'country'] = 'laos'\n",
        "type_idx = meta[meta.sex =='famale'].index\n",
        "meta.loc[type_idx, 'sex'] = 'female'\n",
        "\n",
        "# Delete records with missing audio files\n",
        "missingIdx = meta[meta['file_missing?']==True].index\n",
        "meta.drop(missingIdx, inplace=True )\n",
        "\n",
        "# Delete records with no birthplace - synthesized files\n",
        "meta.dropna(subset=['birthplace'], inplace=True)\n",
        "\n",
        "# Delete files not present in audiofiles database\n",
        "nica_index = meta[meta.filename == 'nicaragua'].index\n",
        "sinhalese_index = meta[meta.filename=='sinhalese1'].index\n",
        "meta.drop(nica_index, inplace=True, axis=0)\n",
        "meta.drop(sinhalese_index, inplace=True, axis=0)\n",
        "\n",
        "meta.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>age_onset</th>\n",
              "      <th>birthplace</th>\n",
              "      <th>filename</th>\n",
              "      <th>native_language</th>\n",
              "      <th>sex</th>\n",
              "      <th>country</th>\n",
              "      <th>file_missing?</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speakerid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>virginia, south africa</td>\n",
              "      <td>afrikaans1</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>female</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>pretoria, south africa</td>\n",
              "      <td>afrikaans2</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>diekabo, ivory coast</td>\n",
              "      <td>agni1</td>\n",
              "      <td>agni</td>\n",
              "      <td>male</td>\n",
              "      <td>ivory coast</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>prishtina, kosovo</td>\n",
              "      <td>albanian1</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>kosovo</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>tirana, albania</td>\n",
              "      <td>albanian2</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>albania</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            age  age_onset  ...       country file_missing?\n",
              "speakerid                   ...                            \n",
              "1          27.0        9.0  ...  south africa         False\n",
              "2          40.0        5.0  ...  south africa         False\n",
              "3          25.0       15.0  ...   ivory coast         False\n",
              "4          19.0        6.0  ...        kosovo         False\n",
              "5          33.0       15.0  ...       albania         False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxfdXrnVKxCY",
        "outputId": "8918821c-96d1-4122-d47e-4010fedbe80c"
      },
      "source": [
        "# Split data into training and testing sets for gender analysis\n",
        "data = meta[['sex', 'filename']]\n",
        "x_train_names, x_test_names, y_train, y_test = train_test_split(data['filename'], \n",
        "                                                                data['sex'], \n",
        "                                                                test_size=0.25, \n",
        "                                                                random_state=38, \n",
        "                                                                stratify=data['sex'])\n",
        "print(x_train_names.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J2r1RxRKyur",
        "outputId": "d5e45c6d-90f6-4f3a-8ca9-fd0f81caafd9"
      },
      "source": [
        "print(x_test_names.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(534,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQeI97MG5x0"
      },
      "source": [
        "Since I can't hold all of the sound files and their augmentations (segmetation, noise addition and VGGish embedding) in memory, I going to write each segement and noise to file, so they can be loaded one by one. I can't figure out how to make this work with tf.Data, since all of the examples use data that is available through TF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUs-hu5YLA5G"
      },
      "source": [
        "# Scale audio to fall between [-1, 1]\n",
        "def normalize(audio):\n",
        "  norm = audio/max(audio)\n",
        "  return norm"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3GVMXSzKV9M"
      },
      "source": [
        "def segment_10s(audio, sr):\n",
        "  \"\"\" Load an audio file and divide into 10 second segments.\n",
        "  Arguments: audio - the audio file; sr = sampling rate of the file\n",
        "  Returns: a dictionary of the audio segments. Key is the index of segment, value is the segment.\n",
        "  \"\"\"\n",
        "  seg_files ={}\n",
        "  n_seg = int((len(audio)/sr)/10)\n",
        "  for i in range(n_seg):\n",
        "    segment = audio[10*i*sr:(i+1)*10*sr]\n",
        "    seg_files[i] = segment\n",
        "  return seg_files"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmX_m9xHYRT"
      },
      "source": [
        "1. Read each audio file in x_train or x_test\n",
        "2. Segment the audio file into 10s segments\n",
        "3. Save each segment to file by appending segment index to filename."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PhrD6U0IMa0"
      },
      "source": [
        "# Original function\n",
        "def segment_data(x_names, y_names, split='train', clf='gender'):\n",
        "  seg_names = []\n",
        "\n",
        "  for i in range(len(x_names)): #df = x_train_names, x_test_names\n",
        "    filename = x_names.iloc[i]\n",
        "    filepath = 'recordings/recordings/' + filename + '.mp3'\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        "    audio = normalize(audio)\n",
        "\n",
        "    # Add gender label to filename, for later processing\n",
        "    sex = y_names.iloc[i]\n",
        "    if sex == \"female\":\n",
        "      filename = '{}.F'.format(filename)\n",
        "    else: filename = '{}.M'.format(filename)\n",
        "\n",
        "    # Segment audio file\n",
        "    seg_files = segment_10s(audio, SAMP_RATE)\n",
        "    for key, val in seg_files.items():\n",
        "      new_name = '{}.{}'.format(filename, key)\n",
        "      sf.write('data/{}/{}/{}o.wav'.format(clf, split, new_name), val, SAMP_RATE)\n",
        "      seg_names.append(new_name)\n",
        "    # end filename\n",
        "  return seg_names"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xocopGN-SR_8"
      },
      "source": [
        "# Refined function\n",
        "def segment_audio(filename, y_value, split='train', clf='gender'):\n",
        "    \"\"\"Load an audio file and segment into 10s increments\n",
        "    Save each segment to the target directory.\n",
        "    Append the gender of the speaker and the segment index to the filename.\n",
        "\n",
        "    Arguments:\n",
        "    filename - base name of audio file (without .mp3 extension)\n",
        "    y_value - class label\n",
        "    split - 'train' or 'test' data, for filepath\n",
        "    clf - 'gender' or 'lang10' for filepath\n",
        "    \"\"\"\n",
        "\n",
        "    filepath = 'recordings/recordings/' + filename + '.mp3'\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        "    audio = normalize(audio)\n",
        "\n",
        "    # Add gender label to filename for later processing\n",
        "    sex = y_value\n",
        "    if sex == 'female':\n",
        "        filename = '{}.F'.format(filename)\n",
        "    else: filename = '{}.M'.format(filename)\n",
        "\n",
        "    # Segment audio file\n",
        "    seg_files = segment_10s(audio, sr)\n",
        "\n",
        "    for key, val in seg_files.items():\n",
        "        new_name = '{}.{}'.format(filename, key)\n",
        "        sf.write('data/{}/{}/{}o.wav'.format(clf, split, new_name), val, sr)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSCzRlGHR64A"
      },
      "source": [
        "# Check if training data has been segmented. If not, segment each audio file.\n",
        "train_file_list = os.listdir('data/gender/train')\n",
        "\n",
        "for i in range(len(x_train_names)):\n",
        "  # get a filename\n",
        "  filename = x_train_names.iloc[i]\n",
        "  # Check to see if the filename has already been segmented\n",
        "  # if any(file.startswith(filename) for file in os.listdir('data/gender/train')):\n",
        "  if any(file.startswith(filename) for file in train_file_list):\n",
        "    pass\n",
        "  else: \n",
        "    augment.Augment.segment_audio(x_train_names.iloc[i], y_train.iloc[i], split='train', clf=CLF)\n",
        "    print('{} segmented'.format(filename))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMZHNvgDStl3"
      },
      "source": [
        "#x_train_seg = segment_audio(x_train_names[:10], y_train, split='train', clf='gender')\n",
        "x_train_seg = [x.split('o.wav')[0] for x in os.listdir('data/gender/train') if x.endswith('o.wav')]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZxCJJjERReK",
        "outputId": "a2af55a5-0515-4525-f3b5-d26fe6b289cf"
      },
      "source": [
        "#x_train_seg = segment_data(x_train_names[:10], y_train, split='train', clf='gender')\n",
        "print(len(x_train_seg))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD80-ZRLZBD3"
      },
      "source": [
        "x_test_seg = [x.split('o.wav')[0] for x in os.listdir('data/gender/test') if x.endswith('o.wav')]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8TMxuNXRrmw",
        "outputId": "d7431f72-4990-458a-af9b-a7b9862ebd0d"
      },
      "source": [
        "#x_test_seg = segment_data(x_test_names[:10], y_test, split='test', clf='gender')\n",
        "print(len(x_test_seg))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJxRO0lLIgRm"
      },
      "source": [
        "Add noise\n",
        "1. Read each audio file (10s segments)\n",
        "2. Add random noise\n",
        "3. Save noisy segement to file by appending noise to filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCT-10IdIuJW"
      },
      "source": [
        "def add_noise(audio):\n",
        "    '''\n",
        "    Add random noise to an audio file.\n",
        "    Arguments: audio - the audio file\n",
        "    Returns: the noisy audio file\n",
        "    ''' \n",
        "    # Load random number generator\n",
        "    rng = np.random.default_rng()\n",
        "    # Generate random noise\n",
        "    noise = rng.standard_normal(len(audio))\n",
        "    # Add noise to file\n",
        "    noisy_seg = audio + 0.005*noise\n",
        "\n",
        "    return noisy_seg"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdSiGpinR1Qe"
      },
      "source": [
        "# Original function\n",
        "def noisy_data_orig(x_names, split='train', clf='gender'):\n",
        "  for i in range(len(x_names)): #list of seg_names\n",
        "    filename = x_names[i]\n",
        "    filepath = 'data/{}/{}/{}o.wav'.format(clf, split, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        " #   audio = normalize(audio) #Already done when originally segmented\n",
        "    # Add noise\n",
        "    noisy = add_noise(audio)\n",
        "    # Write noise to file\n",
        "    sf.write('data/{}/{}/{}n.wav'.format(clf, split, filename), noisy, SAMP_RATE)\n",
        "    print(\"Noise added to {}\".format(x_names[i]))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkT1m_GvXu0W"
      },
      "source": [
        "# Revised function\n",
        "def noisy_data(filename, split='train', clf='gender'):\n",
        "    \"\"\"Load an audio file (or segment).   \n",
        "    Add random noise to the file and save with new filename.\n",
        "\n",
        "    Arguments:\n",
        "    filename - filename/segment base, without 'o.wav'\n",
        "    split - 'train' or 'test' data, for filepath\n",
        "    clf - 'gender' or 'lang10' for filepath\n",
        "    \"\"\"\n",
        "\n",
        "    filepath = 'data/{}/{}/{}o.wav'.format(clf, split, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        " \n",
        "    # Add noise\n",
        "    noisy = add_noise(audio)\n",
        "    # Write noise to file\n",
        "    sf.write('data/{}/{}/{}n.wav'.format(clf, split, filename), noisy, sr)\n",
        "    #print(\"Noise added to {}\".format(filename))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2u2YhjwXrpV"
      },
      "source": [
        "# Check if training data has been augmented with noise. If not, add noise to each segment.\n",
        "noise_train_list = os.listdir('data/gender/train')\n",
        "for i in range(len(x_train_seg)):\n",
        "  filename = x_train_seg[i]\n",
        "  # if any((file.startswith(filename)& file.endswith('n.wav')) for file in os.listdir('data/gender/train')):\n",
        "  if any((file.startswith(filename)& file.endswith('n.wav')) for file in noise_train_list):\n",
        "    pass\n",
        "  else: \n",
        "    augment.Augment.noisy_data(x_train_seg[i], split='train', clf=CLF)\n",
        "    print('{} augmented'.format(filename))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc5SZEz8YB1M",
        "outputId": "5f83165d-078e-4b24-e98a-04c359c25342"
      },
      "source": [
        "# Verify there are equal numbers for original segments and noisy segments.\n",
        "x_train_noise = [x.split('n.wav')[0] for x in os.listdir('data/gender/train') if x.endswith('n.wav')]\n",
        "print(len(x_train_seg) == len(x_train_noise))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcH0tkBDkNHP"
      },
      "source": [
        "# Generate noisy samples\n",
        "# noisy_data(x_train_seg[:15], split='train', clf='gender')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7TaCs4WkfVC"
      },
      "source": [
        "# No need to add noise to test files!\n",
        "#noisy_data(x_test_seg[:10], split='test', clf='gender')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WogZrOwRkhRz"
      },
      "source": [
        "#!ls data/gender/train/"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1FjJOw0kqG6"
      },
      "source": [
        "#!ls data/gender/test/"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBJeBKHoUdc7",
        "outputId": "0bd2a220-6f65-4db4-abe3-363090ac7502"
      },
      "source": [
        "x_train_filenames = os.listdir('./data/gender/train')\n",
        "x_train_filenames = x_train_filenames[:20]\n",
        "print(x_train_filenames)\n",
        "\n",
        "x_train_filepaths = ['./data/gender/train/{}'.format(i) for i in x_train_filenames]\n",
        "print(len(x_train_filepaths))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['english167.M.0n.wav', 'english167.M.1n.wav', 'english66.M.0n.wav', 'english66.M.1n.wav', 'hausa6.M.0n.wav', 'hausa6.M.1n.wav', 'susu1.M.0n.wav', 'susu1.M.1n.wav', 'german8.M.0n.wav', 'tamil6.F.0n.wav', 'tamil6.F.1n.wav', 'english171.M.0n.wav', 'czech5.F.0n.wav', 'czech5.F.1n.wav', 'czech5.F.2n.wav', 'portuguese5.M.0n.wav', 'portuguese5.M.1n.wav', 'portuguese5.M.2n.wav', 'english275.F.0n.wav', 'english275.F.1n.wav']\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYAF9vcyAvk",
        "outputId": "21afb26e-2869-425e-def0-beeb77b86ce4"
      },
      "source": [
        "x_test_filenames = os.listdir('./data/gender/test')\n",
        "x_test_filenames = x_test_filenames[:10]\n",
        "print(x_test_filenames)\n",
        "\n",
        "x_test_filepaths = ['./data/gender/test/{}'.format(i) for i in x_test_filenames]\n",
        "print(len(x_test_filepaths))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['spanish130.M.0o.wav', 'spanish130.M.1o.wav', 'spanish12.M.0o.wav', 'spanish12.M.1o.wav', 'spanish12.M.2o.wav', 'spanish12.M.3o.wav', 'spanish12.M.4o.wav', 'dutch29.M.0o.wav', 'dutch29.M.1o.wav', 'pashto1.F.0o.wav']\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOATRYRKIc5Q"
      },
      "source": [
        "Convert to VGGish embedding\n",
        "1. Load VGGish model\n",
        "1. Read each audio file (10s segments with and without noise)\n",
        "1. Run through VGGish model\n",
        "1. Save embedding by appending _embed to filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqw7s0amUV-U"
      },
      "source": [
        "# Using a SavedModel from the TFHub in Keras\n",
        "# https://www.tensorflow.org/hub/tf2_saved_model\n",
        "# VGGish model, from https://tfhub.dev/google/vggish/1\n",
        "\n",
        "# Link to the model on TFHub\n",
        "hub_url = 'https://tfhub.dev/google/vggish/1'\n",
        "\n",
        "# Load the model as a Keras model\n",
        "vggish_model = hub.KerasLayer(hub_url)\n",
        "vggish_model.trainable = False\n",
        "\n",
        "# Load the model as a tf.function - not needed, since it doesn't work\n",
        "#vggish_fn = hub.load(hub_url)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpFjpxAX3YMU"
      },
      "source": [
        "# Define transformation function and dataset generator\n",
        "\n",
        "Adapted from https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-WwlXbp3j_0"
      },
      "source": [
        "#def vggish_transform(audio):\n",
        "#  return vggish_fn(audio)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVfe-g1PW4Rt"
      },
      "source": [
        "# Original function\n",
        "# Write a generator to read data in chunks and process it\n",
        "# Generator yields both data and labels\n",
        "# Takes a list of filenames as first argument, batch_size as second argument\n",
        "\n",
        "#https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/\n",
        "\n",
        "def tf_data_generator_orig(file_list, batch_size=32):\n",
        "  i = 0\n",
        "  while True: #infinite loop\n",
        "    if i*batch_size >= len(file_list):\n",
        "      i=0\n",
        "      np.random.shuffle(file_list)\n",
        "    else:\n",
        "      file_chunk = file_list[i*batch_size:(i+1)*batch_size]\n",
        "      data = []\n",
        "      labels = []\n",
        "      label_classes = tf.constant(['M', 'F'])\n",
        "      for file in file_chunk:\n",
        "        # Read data\n",
        "        audio, sr = librosa.load(file, sr=16000)\n",
        "        # Apply transformations\n",
        "        embed = vggish_model(audio)\n",
        "        data.append(embed)\n",
        "        #data.append(audio)\n",
        "        # Extract labels from filename\n",
        "        bytes_string = file\n",
        "        string_name = str(bytes_string, 'utf-8')\n",
        "        split_str = string_name.split('.')\n",
        "        pattern = tf.constant(split_str[2])\n",
        "        for j in range(len(label_classes)):\n",
        "          if re.match(pattern.numpy(), label_classes[j].numpy()):\n",
        "            labels.append(j)\n",
        "\n",
        "      data = np.asarray(data)\n",
        "      labels = np.asarray(labels)\n",
        "\n",
        "      # To be able t prefecth the data you can use the mpa function, \n",
        "      # but this doesn't work for VGGish, sincc VGGish only processes one file at a time\n",
        "      #first_dim = data.shape[0]\n",
        "      ## Create tensorflow dataset to use 'map' function for parallelization\n",
        "      #data_ds = tf.data.Dataset.from_tensor_slices(data)\n",
        "      #data_ds = data_ds.batch(batch_size = first_dim).map(vggish_transform,\n",
        "                                                         # num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "      # Convert dataset to generator and subsequently to np array\n",
        "      #data_ds = tfds.as_numpy(data_ds)\n",
        "      #data = np.array([data for data in data_ds]).reshape(first_dim, 10, 128)\n",
        "\n",
        "      yield data, labels\n",
        "      i += 1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj7zSmWsaHJ7"
      },
      "source": [
        "def tf_data_generator(file_list, batch_size=32):\n",
        "    \"\"\" Create a dataset generator. \n",
        "    Iterate through a list of filenames and process in batches.\n",
        "    Extract audio features from vggish model.\n",
        "    WARNING: This generator forms an infinite loop, \n",
        "    so you need to specify how long to run the generator \n",
        "    before fitting and evaluating a model.\n",
        "\n",
        "    Arguments:\n",
        "    file_list - list of filenames to iterate\n",
        "    vggish_model  - pass the instantiated model to the function\n",
        "    batch_size - how many files to process at a time\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    while True: #infinite loop\n",
        "        if i*batch_size >= len(file_list):\n",
        "            i=0\n",
        "            np.random.shuffle(file_list)\n",
        "        else:\n",
        "            file_chunk = file_list[i*batch_size:(i+1)*batch_size]\n",
        "            data = []\n",
        "            labels = []\n",
        "            label_classes = tf.constant(['M', 'F'])\n",
        "            for file in file_chunk:\n",
        "                # Read data\n",
        "                audio, sr = librosa.load(file, sr=16000)\n",
        "                # Apply transformations\n",
        "                embed = vggish_model(audio)\n",
        "                data.append(embed)\n",
        "                # Extract labels from filename\n",
        "                bytes_string = file\n",
        "                string_name = str(bytes_string, 'utf-8')\n",
        "                split_str = string_name.split('.')\n",
        "                pattern = tf.constant(split_str[2])\n",
        "                for j in range(len(label_classes)):\n",
        "                    if re.match(pattern.numpy(), label_classes[j].numpy()):\n",
        "                        labels.append(j)\n",
        "\n",
        "            data = np.asarray(data)\n",
        "            labels = np.asarray(labels)\n",
        "\n",
        "            yield data, labels\n",
        "            i += 1"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhzcoHwjjHsr"
      },
      "source": [
        "dataset_check = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_train_filepaths[:12], BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQmW_OYYjkG9",
        "outputId": "6938ca53-070d-4206-b9bc-e485638a0c19"
      },
      "source": [
        "for data, labels in dataset_check.take(2):\n",
        "  print(data.shape)\n",
        "  print(labels)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 10, 128)\n",
            "tf.Tensor([0. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)\n",
            "(6, 10, 128)\n",
            "tf.Tensor([0. 0. 0. 1. 1. 0.], shape=(6,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEXOQAav2kB"
      },
      "source": [
        "# Create pipeline and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Pk8AuYv1G4"
      },
      "source": [
        "x_train, x_val = train_test_split(x_train_filepaths, test_size=.25, random_state=38)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2zXm1KIw1Jq",
        "outputId": "28ad4266-3d49-46f4-9d1f-48c089277ff9"
      },
      "source": [
        "# Print sizes of data splits\n",
        "print(\"Number of training samples: \", len(x_train))\n",
        "print(\"Number of vaidation samples: \", len(x_val))\n",
        "print(\"Number of testing samples: \", len(x_test_seg))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  15\n",
            "Number of vaidation samples:  5\n",
            "Number of testing samples:  1206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuO150FTyPnp",
        "outputId": "3166c018-2728-4679-b54e-c5cc840e90bf"
      },
      "source": [
        "print(BATCH_SIZE)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVn4nKKoxkrf"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_train, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) ) \n",
        "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_val, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) )\n",
        "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [x_test_filepaths, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,)) ) "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGIx7TvhFtO6",
        "outputId": "7dc00c81-8e1b-458f-d8e8-1b74f18b64d8"
      },
      "source": [
        "# Check structure of datasets, with the goal of extracting the labels\n",
        "\n",
        "# Look at each type of element component\n",
        "test_dataset.element_spec"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kryXvX26e9d"
      },
      "source": [
        "#I don't think this will work, since I can't get VGGish to fuction on multiple files at a time\n",
        "# Prefetch datasets = prepare next batch with CPU while GPU trains on previous batch\n",
        "#train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "#validation_dataset = validation_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxkrPsFeyqoG"
      },
      "source": [
        "# Build and compile the model\n",
        "Copy from other notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNVZkSI8F0_"
      },
      "source": [
        "genderClf = tf.keras.models.Sequential([tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "                              tf.keras.layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\", data_format=\"channels_last\")\n",
        "                              ])\n",
        "genderClf.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIcjLruK8H_V"
      },
      "source": [
        "# Add early stopping to train classifier model; default is 10 epochs\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping_monitor = EarlyStopping(patience=2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1xgA1Vjyyku"
      },
      "source": [
        "*Important* before fitting model, specify number of epochs and steps to fit, since generators are infinite loops.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAoyS3n1zmW0",
        "outputId": "0c7650f5-c177-4850-d2d2-066e45653aaf"
      },
      "source": [
        "# Calculate how many dataset batches to generate, since generator is infinite\n",
        "steps_per_epoch = np.int(np.ceil(len(x_train)/BATCH_SIZE))\n",
        "val_steps = np.int(np.ceil(len(x_val)/BATCH_SIZE))\n",
        "eval_steps = np.int(np.ceil(len(x_test_filepaths)/BATCH_SIZE))\n",
        "\n",
        "print(\"steps_per_epoch = \", steps_per_epoch)\n",
        "print(\"validation_steps = \", val_steps)\n",
        "print(\"steps = \", eval_steps)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "steps_per_epoch =  3\n",
            "validation_steps =  1\n",
            "steps =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIW93ghK0oS5",
        "outputId": "152c719f-39e6-48e9-f23d-aa7a648c1f3f"
      },
      "source": [
        "#model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
        "#         validation_steps = val_steps, epochs = 5)\n",
        "\n",
        "history = genderClf.fit(train_dataset,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        epochs=20,\n",
        "                        validation_data=validation_dataset,\n",
        "                        validation_steps = val_steps,\n",
        "                        callbacks=[early_stopping_monitor], \n",
        "                        batch_size=BATCH_SIZE)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.6711 - accuracy: 0.6667 - val_loss: 0.5551 - val_accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.5072 - accuracy: 0.6667 - val_loss: 0.4500 - val_accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.4047 - accuracy: 0.7333 - val_loss: 0.3483 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.3084 - accuracy: 1.0000 - val_loss: 0.2523 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.2333 - accuracy: 1.0000 - val_loss: 0.1740 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.1627 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.1185 - accuracy: 1.0000 - val_loss: 0.0843 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 9s 3s/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JymMio95636G",
        "outputId": "b83d3a70-b899-448a-cf76-12a685ef1d1e"
      },
      "source": [
        "# Evaluate model?\n",
        "test_loss, test_acc = genderClf.evaluate(test_dataset, steps=eval_steps)\n",
        "#print(test_loss)\n",
        "#print(test_acc)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 4s 2s/step - loss: 0.0103 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izdCh1mt7cMI"
      },
      "source": [
        "# Making predictions\n",
        "y_pred = genderClf.predict(test_dataset, steps=eval_steps)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX4ouOufFf3_",
        "outputId": "d0d790f5-981e-4da3-dfda-9c9e52b44c33"
      },
      "source": [
        "y_test_labels = []\n",
        "for data, labels in test_dataset.take(eval_steps):\n",
        "  y_test_labels.append(labels.numpy())\n",
        "\n",
        "print(type(y_test_labels))\n",
        "print(len(y_test_labels))\n",
        "print(y_test_labels)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "2\n",
            "[array([0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 1.], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn5YwvTHK_42",
        "outputId": "80dc2fd0-064a-4080-d05d-dbf5059119bd"
      },
      "source": [
        "print(y_pred.shape)\n",
        "y_pred = y_pred [:, 0, 0]\n",
        "print(y_pred.shape)\n",
        "#print(y_pred)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 1, 1)\n",
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmmI4-F6LZZ_",
        "outputId": "0687fe7e-281d-4364-d498-1536ea680748"
      },
      "source": [
        "y_pred_int  = []\n",
        "for i in y_pred:\n",
        "  if i < 0.5:\n",
        "    y_pred_int.append(0)\n",
        "  else: y_pred_int.append(1)\n",
        "\n",
        "print(y_pred_int)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFgg_CWBYkg"
      },
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "To create a dataset of all files matching a pattern, use tf.data.Dataset.list_files:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOxdxJM4BVX4"
      },
      "source": [
        "#dataset = tf.data.Dataset.list_files(\"/path/*.txt\")  # doctest: +SKIP"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106Hn5J_CX2G"
      },
      "source": [
        "batch\n",
        "View source\n",
        "\n",
        "batch(\n",
        "    batch_size, drop_remainder=False\n",
        ")\n",
        "\n",
        "Combines consecutive elements of this dataset into batches.\n",
        "\n",
        "dataset = tf.data.Dataset.range(8)\n",
        "dataset = dataset.batch(3)\n",
        "list(dataset.as_numpy_iterator())\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.range(8)\n",
        "dataset = dataset.batch(3, drop_remainder=True)\n",
        "list(dataset.as_numpy_iterator())\n",
        "\n",
        "\n",
        "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DSYfePD-zV"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/audio/simple_audio\n",
        "\n",
        "Simple audio recognition: Recognizing keywords"
      ]
    }
  ]
}