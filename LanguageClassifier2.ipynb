{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageClassifier2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x9qy1QSq9y04",
        "hgmmbsTqluFe",
        "wLNY7Lv7l_UD",
        "b6C7mMUKtDwu",
        "Ad0LnejymVHJ"
      ],
      "authorship_tag": "ABX9TyNpHhOT83oeIgFMpWnEj8zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kregier/AudioLanguageClassifer/blob/main/LanguageClassifier2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPwQ75UMELA"
      },
      "source": [
        "# Introduction\n",
        "This notebook contains a third attempt at a language classifier model, which includes 3 dense layers, with a 50% dropout layer between the first two layers. However, the kernel crashes when I try to train this model, and I don't have any idea why. \n",
        "\n",
        "This classifier identifies the native language of the speaker from an English reading.\n",
        "\n",
        "Only the top 10 langauges (in addition to English) from the Speech Sample archive are included.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OJdVw7Tjn46"
      },
      "source": [
        "## Set up the environment and load the metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFFe4-bHL-Vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2f8223-965c-48f4-df1d-e50d81ada35c"
      },
      "source": [
        "# Set up the environment\n",
        "!pip install soundfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from keras.layers import Dense, Input, Flatten, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"All set up!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "All set up!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UhmzjUR-ZNe"
      },
      "source": [
        "#https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVpYignFM10w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3ac398-5d82-43c7-a808-461f588b7809"
      },
      "source": [
        "# Set up the data import using Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsQGgd1XM2jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb035bd3-3214-447e-b352-05e71336199e"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "\n",
        "# Change working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n",
            "augment  kaggle.json  processed.csv\t   recordings\n",
            "data\t model\t      reading-passage.txt  speakers_all.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68jAGi9XM423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a61c3b07-8e38-4701-aeea-f54d54707f43"
      },
      "source": [
        "# Import custom functions that I wrote\n",
        "import augment\n",
        "from augment import Augment\n",
        "\n",
        "from imp import reload"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module imported\n",
            "Augment scripts reloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ith_yV0xM-yE"
      },
      "source": [
        "# Set constants\n",
        "SAMP_RATE = 16000  #Defined in augment package\n",
        "BATCH_SIZE = 32  #Defined in augment package\n",
        "CLF = 'lang10'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NPgaHp7NEHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "39b2d37e-d26f-498e-b34a-ce34e1def093"
      },
      "source": [
        "meta = pd.read_csv('processed.csv', index_col='speakerid')\n",
        "meta.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>age_onset</th>\n",
              "      <th>birthplace</th>\n",
              "      <th>filename</th>\n",
              "      <th>native_language</th>\n",
              "      <th>sex</th>\n",
              "      <th>country</th>\n",
              "      <th>file_missing?</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speakerid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>virginia, south africa</td>\n",
              "      <td>afrikaans1</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>female</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>pretoria, south africa</td>\n",
              "      <td>afrikaans2</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>diekabo, ivory coast</td>\n",
              "      <td>agni1</td>\n",
              "      <td>agni</td>\n",
              "      <td>male</td>\n",
              "      <td>ivory coast</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>prishtina, kosovo</td>\n",
              "      <td>albanian1</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>kosovo</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>tirana, albania</td>\n",
              "      <td>albanian2</td>\n",
              "      <td>albanian</td>\n",
              "      <td>male</td>\n",
              "      <td>albania</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            age  age_onset  ...       country file_missing?\n",
              "speakerid                   ...                            \n",
              "1          27.0        9.0  ...  south africa         False\n",
              "2          40.0        5.0  ...  south africa         False\n",
              "3          25.0       15.0  ...   ivory coast         False\n",
              "4          19.0        6.0  ...        kosovo         False\n",
              "5          33.0       15.0  ...       albania         False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbA2HHQulWYZ"
      },
      "source": [
        "# Create a data generator and generate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9qy1QSq9y04"
      },
      "source": [
        "## Load the VGGish model\n",
        "Needs to be instantiated before dataset generation can be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohD0F3OS5tXy"
      },
      "source": [
        "# Using a SavedModel from the TFHub in Keras\n",
        "# https://www.tensorflow.org/hub/tf2_saved_model\n",
        "# VGGish model, from https://tfhub.dev/google/vggish/1\n",
        "\n",
        "# Link to the model on TFHub\n",
        "hub_url = 'https://tfhub.dev/google/vggish/1'\n",
        "\n",
        "# Load the model as a Keras model\n",
        "vggish_model = hub.KerasLayer(hub_url)\n",
        "vggish_model.trainable = False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDb5lqe9_HA"
      },
      "source": [
        "## Create lists of filenames and paths to feed the generators\n",
        "Following the example from https://keras.io/examples/audio/speaker_recognition_using_cnn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGXpQIKRCBEi",
        "outputId": "b183ed16-744e-4e5f-d867-7eec61f821f7"
      },
      "source": [
        "# Dataset generation from Keras.IO example\n",
        "# https://keras.io/examples/audio/speaker_recognition_using_cnn/\n",
        "\n",
        "from pathlib import Path\n",
        "# Get the list of audio file paths along with their corresponding labels\n",
        "DATASET_ROOT = \"data/lang10\"\n",
        "TRAIN_SUBFOLDER = \"train\"\n",
        "VAL_SUBFOLDER = \"validation\"\n",
        "TEST_SUBFOLDER = \"test/test_data\"\n",
        "\n",
        "DATASET_TRAIN_PATH = os.path.join(DATASET_ROOT, TRAIN_SUBFOLDER)\n",
        "DATASET_VAL_PATH = os.path.join(DATASET_ROOT, VAL_SUBFOLDER)\n",
        "DATASET_TEST_PATH = os.path.join(DATASET_ROOT, TEST_SUBFOLDER)\n",
        "\n",
        "SHUFFLE_SEED = 38\n",
        "SAMP_RATE = 16000\n",
        "\n",
        "class_names = os.listdir(DATASET_TRAIN_PATH)\n",
        "print(\"Our class names: {}\".format(class_names,))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our class names: ['spanish', 'arabic', 'mandarin', 'french', 'korean', 'russian', 'portuguese', 'dutch', 'turkish', 'german', 'english']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc5m5KDor-A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d8ff2b-f019-45da-a543-548779d61b22"
      },
      "source": [
        "# Process training data\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing language {}\".format(name,))\n",
        "    dir_path = Path(DATASET_TRAIN_PATH) / name\n",
        "    language_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    train_paths += language_sample_paths\n",
        "    train_labels += [label] * len(language_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(train_paths), len(class_names))\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing language spanish\n",
            "Processing language arabic\n",
            "Processing language mandarin\n",
            "Processing language french\n",
            "Processing language korean\n",
            "Processing language russian\n",
            "Processing language portuguese\n",
            "Processing language dutch\n",
            "Processing language turkish\n",
            "Processing language german\n",
            "Processing language english\n",
            "Found 1929 files belonging to 11 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwii0YwLSX5T",
        "outputId": "6118d7ad-b1af-4ac5-994f-c1fa96bcef1f"
      },
      "source": [
        "print(train_labels[:2*BATCH_SIZE])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dym3_rDXaI",
        "outputId": "2fc10e45-5fe9-4dcf-fb6c-8d37cd269231"
      },
      "source": [
        "# Process validation data\n",
        "validation_paths = []\n",
        "val_labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing language {}\".format(name,))\n",
        "    dir_path = Path(DATASET_VAL_PATH) / name\n",
        "    language_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    validation_paths += language_sample_paths\n",
        "    val_labels += [label] * len(language_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(validation_paths), len(class_names))\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing language spanish\n",
            "Processing language arabic\n",
            "Processing language mandarin\n",
            "Processing language french\n",
            "Processing language korean\n",
            "Processing language russian\n",
            "Processing language portuguese\n",
            "Processing language dutch\n",
            "Processing language turkish\n",
            "Processing language german\n",
            "Processing language english\n",
            "Found 652 files belonging to 11 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFWanBGxtiW8"
      },
      "source": [
        "# Process testing data\n",
        "testing_filenames = os.listdir(DATASET_TEST_PATH)\n",
        "testing_paths = []\n",
        "test_labels = []\n",
        "\n",
        "label_dict = {}\n",
        "for label, name in enumerate(class_names):\n",
        "  label_dict[name] = label\n",
        "\n",
        "for filename in testing_filenames:\n",
        "  filepath =  os.path.join(DATASET_TEST_PATH, filename)\n",
        "  testing_paths.append(filepath)\n",
        "  fname = filename.split('.')[0].rstrip('0123456789')\n",
        "  test_labels.append(label_dict[fname])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEO_EI3fD4m7"
      },
      "source": [
        "# Shuffle the order of the traning and validation datasets and labels\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(train_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(train_labels)\n",
        "\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(validation_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(val_labels)\n",
        "\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(testing_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(test_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmmbsTqluFe"
      },
      "source": [
        "## One-hot encode the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj0DAd_vtdpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34be4e4-1dc8-4d07-96e1-7246841555b7"
      },
      "source": [
        "train_onehot = tf.keras.backend.one_hot(train_labels, num_classes = len(class_names))\n",
        "val_onehot = tf.keras.backend.one_hot(val_labels, num_classes = len(class_names))\n",
        "test_onehot = tf.keras.backend.one_hot(test_labels, num_classes = len(class_names))\n",
        "\n",
        "print(type(train_onehot) )\n",
        "print(train_onehot.shape)\n",
        "print(train_onehot[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(1929, 11)\n",
            "tf.Tensor([1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(11,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLNY7Lv7l_UD"
      },
      "source": [
        "## Get sample sizes and number of batches for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkpHUMnj6UU9",
        "outputId": "f700a327-a385-4b66-e792-09d6fb8dc591"
      },
      "source": [
        "# Print sizes of data splits\n",
        "print(\"Number of training samples: \", len(train_paths))\n",
        "print(\"Number of validation samples: \", len(validation_paths))\n",
        "print(\"Number of testing samples: \", len(testing_paths))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  1929\n",
            "Number of validation samples:  652\n",
            "Number of testing samples:  446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35YWIPqd7BCK",
        "outputId": "7fccd224-1f9b-4f67-81a5-359a4afb9ccf"
      },
      "source": [
        "# Calculate how many dataset batches to generate\n",
        "train_steps = np.int(np.ceil(len(train_paths)/BATCH_SIZE)) -1\n",
        "val_steps = np.int(np.ceil(len(validation_paths)/BATCH_SIZE))-1\n",
        "eval_steps = np.int(np.ceil(len(testing_paths)/BATCH_SIZE))-1\n",
        "\n",
        "print(\"training_steps_per_epoch = \", train_steps)\n",
        "print(\"validation_steps = \", val_steps)\n",
        "print(\"evaluation_steps = \", eval_steps)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_steps_per_epoch =  60\n",
            "validation_steps =  20\n",
            "evaluation_steps =  13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6C7mMUKtDwu"
      },
      "source": [
        "## Define dataset generator\n",
        "Then, run a small test dataset, to confirm input dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Luq-GPtDeS"
      },
      "source": [
        "def tf_data_generator(file_list, label_list, batch_size=32):\n",
        "    \"\"\" Create a dataset generator. \n",
        "    Iterate through a list of filenames and process in batches.\n",
        "    Extract audio features from vggish model.\n",
        "    WARNING: This generator forms an infinite loop, \n",
        "    so you need to specify how long to run the generator \n",
        "    before fitting and evaluating a model.\n",
        "\n",
        "    Arguments:\n",
        "    file_list - list of filenames to iterate\n",
        "    label_list - list of the labels associated with the files\n",
        "    batch_size - how many files to process at a time\n",
        "    \"\"\"\n",
        "    i = 1\n",
        "    shuff_seed = 42\n",
        "    while True: #infinite loop\n",
        "        if i*batch_size >= len(file_list):\n",
        "            i=1\n",
        "            rng = np.random.RandomState(shuff_seed)\n",
        "            rng.shuffle(file_list)\n",
        "            rng = np.random.RandomState(shuff_seed)\n",
        "            rng.shuffle(label_list)\n",
        "            shuff_seed += 1\n",
        "        else:\n",
        "            file_chunk = file_list[(i-1)*batch_size:i*batch_size]\n",
        "            labels = label_list[(i-1)*batch_size:i*batch_size, :]\n",
        "            data = []\n",
        "\n",
        "            for file in file_chunk:\n",
        "                # Read data\n",
        "                audio, sr = librosa.load(file, sr=16000)\n",
        "                # Apply transformations\n",
        "                embed = vggish_model(audio)\n",
        "                data.append(embed)\n",
        "                # Extract labels from filename\n",
        "\n",
        "            data = np.asarray(data)\n",
        "            labels = np.asarray(labels)\n",
        "\n",
        "            yield data, labels\n",
        "            i += 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm1zl_dAuyFM"
      },
      "source": [
        "dataset_check = tf.data.Dataset.from_generator(tf_data_generator,\n",
        "                                         args = [train_paths[:2*BATCH_SIZE], train_onehot[:2*BATCH_SIZE], BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,11)) )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4emugYzuu-Dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4bd28b-d405-41ab-aeee-6e3685a73df6"
      },
      "source": [
        "# Check shape and size of dataset batches\n",
        "for data, labels in dataset_check.take(2):\n",
        "  print(data.shape)\n",
        "  print(labels.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10, 128)\n",
            "(32, 11)\n",
            "(32, 10, 128)\n",
            "(32, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad0LnejymVHJ"
      },
      "source": [
        "## Generate the datasets for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQam7y2vF1U"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [train_paths, train_onehot, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,11)) ) \n",
        "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [validation_paths, val_onehot, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,11)) )\n",
        "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, \n",
        "                                         args = [testing_paths, test_onehot, BATCH_SIZE],\n",
        "                                         output_types=(tf.float32, tf.float32),\n",
        "                                         output_shapes= ((None, 10, 128),(None,11)) ) "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4hH547nvtM"
      },
      "source": [
        "# Model 3 - 3 dense layers, with dropout\n",
        "\n",
        "A dropout layer has been added after the first dense layer, in an attempt to avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9UICQSGm2hT"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgMTKhyK_XxF"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "def layer3_drop():\n",
        "  model = Sequential()\n",
        "  #Input shape (None, 10, 128)\n",
        "  model.add(Input(shape=(10, 128), batch_size=BATCH_SIZE )) \n",
        "  model.add(Dense(128, activation = 'relu' ) ) #, input_shape=(10, 128) ) )\n",
        "  model.add(Dense(.5) )\n",
        "  model.add(Dense(64, activation = 'relu' ) ) #, input_shape=(10, 128) ) )\n",
        "  model.add(Flatten())\n",
        "  # Output\n",
        "  model.add(Dense(11, activation='softmax') )\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "lang_3dense_drop = layer3_drop()\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFJRKtRoc-A5"
      },
      "source": [
        "Add checkpoint callback to model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6mBqMqiXkF"
      },
      "source": [
        "ckpt_path = 'model/lang10/checkpoints/lang_3dense_drop.ckpt'\n",
        "ckpt_dir = os.path.dirname(ckpt_path)\n",
        "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=ckpt_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyoJodP1_We0"
      },
      "source": [
        "# Add early stopping to train classifier model; default is 10 epochs\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping_monitor = EarlyStopping(patience=2)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-VduLqEm61d"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfhupD05ihMq",
        "outputId": "2f06adc3-7e33-4171-f8b2-f5c5ee0c45e4"
      },
      "source": [
        "# Fit the classifier\n",
        "#if os.path.isdir('model/lang10/lang_3dense_drop/'):\n",
        "#  lang_3dense_drop = tf.keras.models.load_model('model/lang10/lang_3dense_drop')\n",
        "#  hist_3drop_df = pd.read_csv('model/lang10/lang_3dense_drop.history.csv')\n",
        "#else:\n",
        "#  if os.path.isfile(ckpt_dir):\n",
        "    # Load model weights from the most recent checkpoint\n",
        "#    latest = tf.train.latest_checkpoint(ckpt_dir)\n",
        "#    lang_3dense_drop.load_weights(latest)\n",
        "hist_3drop = lang_3dense_drop.fit(train_dataset, \n",
        "                        steps_per_epoch=train_steps, \n",
        "                        epochs=20, \n",
        "                        validation_data=validation_dataset, \n",
        "                        validation_steps = val_steps,\n",
        "                        callbacks=[early_stopping_monitor, ckpt]) #, \n",
        "                        #batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNHU-BmOi07K"
      },
      "source": [
        "# Save the trained model and model history for use later\n",
        "lang_3dense_drop.save('model/lang10/lang_3dense_drop')\n",
        "\n",
        "hist_3drop_df = pd.DataFrame(hist_3drop.history) \n",
        "\n",
        "# save to csv: \n",
        "hist_csv_file = 'model/lang10/lang_3dense_drop.history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_3dense_df.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdPE3eWEAwJG"
      },
      "source": [
        "## Trained model summary information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHA8bcujFpx"
      },
      "source": [
        "lang_3dense_drop.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swfomPxHxyHa"
      },
      "source": [
        "hist_3drop_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o-Li_1MAzUW"
      },
      "source": [
        "plt.plot(hist_3drop_df['accuracy'])\n",
        "plt.plot(hist_3drop_df['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qv6QlKFA5UM"
      },
      "source": [
        "plt.plot(hist_3drop_df['loss'])\n",
        "plt.plot(hist_3drop_df['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP-Qhpm9BDlO"
      },
      "source": [
        "## Evaluate the trained classifier\n",
        "Based on the methods used fro the Gender classifier; probably need to be refined, but let's look for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns36EM_4BVB5"
      },
      "source": [
        "# Evaluate on validation set\n",
        "val_loss_3dr, val_acc_3dr = lang_3dense_drop.evaluate(validation_dataset, steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_79D7njByKs"
      },
      "source": [
        "# Evaluate on testing set\n",
        "test_loss_3dr, test_acc_3dr = lang_3dense_drop.evaluate(test_dataset, steps=eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgDWJX0DB02h"
      },
      "source": [
        "# Predict testing set\n",
        "y_pred_3dr = lang_3dense_drop.predict(test_dataset, steps=eval_steps)\n",
        "print(y_pred_3dr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJQjdFCMqPl"
      },
      "source": [
        "print(y_pred_3dr[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WglAgLVbMvt3"
      },
      "source": [
        "# Get label index using np.argmax from each prediction\n",
        "y_pred_ind_3dr = np.argmax(y_pred, axis=1)\n",
        "print(y_pred_ind_3dr[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Zdnqm50TBY"
      },
      "source": [
        "# Get np.argmax from each prediction as label prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gzmes4PEP8y"
      },
      "source": [
        "con_matrix_3dr = tf.math.confusion_matrix(test_labels[:y_pred_3dr.shape[0]], y_pred_ind_3dr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWlxZZRhEal4"
      },
      "source": [
        "sns.heatmap(con_matrix_3dr, annot = True, cmap='YlGnBu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcJtkg_IZ0T"
      },
      "source": [
        "# Other model building steps:\n",
        "- Add drop out layers\n",
        "- Add more dense layers\n",
        "\n",
        "\n",
        "What is a good accuracy rate for multi-class classification?\n",
        "\n"
      ]
    }
  ]
}