\documentclass[11pt, letterpaper]{article}
\usepackage{palatino}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[top=1in, left=1in, right=1in, bottom=1in]{geometry}
% - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - - 
\begin{document}
\begin{center}
\Large{\textbf{Audio Language Classifier:}}

\Large{\textbf{Model Metrics}}

\large{Kirsten Regier}
\end{center}

\section{Gender Classifier}
\subsection{Model Architecture}

Two different models were trained and evaluated for the gender classifier. The structure of each model is shown in Table \ref{tab:GenModels}.  Model 1 has one hidden layer with 128 nodes, while Model 2 has two hidden layers with 128 and 64 nodes, respectively.

\begin{table}[!h]
\begin{center}
\caption{Gender Classifier Model Architectures.}
\begin{tabular}{l | c | c |}

Layer  & Model 1 & Model 2\\
\hline

Input 	& (32, 10, 128) & (32, 10, 128) \\ \hline

Dense	& 128 nodes & 128 nodes \\
Dropout	& 50\%		& 50 \% \\ \hline

Dense	&			& 64 nodes \\
Dropout	& 			& 50\% \\ \hline

Flatten 	& (32, 1280)	& (32, 640) \\ \hline
Output 	& (32, 1)		& (32, 1)\\
\hline
\end{tabular}

\label{tab:GenModels}
\end{center}
\end{table}

\subsection{Model Performance Comparison}

\begin{table}[h]
\begin{center}
\caption{Gender Classifiers - Metrics Comparison}
\begin{tabular}{l c c}
& 	Model 1 & Model 2 \\ \hline
Loss	&0.061640 & 0.076962 \\
Accuracy& 0.981419 & 0.975507 \\
Precision & 0.982818 & 0.960199 \\
Recall & 0.979452 & 0.991438 \\
\end{tabular}
\label{tab:GenMetricsSum}
\end{center}
\end{table} 

\subsection{Confusion Matrices}

%The confusion matrices for both models are shown in Table \ref{tab:GenConfusion}.

\begin{table}[h!]
\begin{center}
\caption{Confusion Matrices for the Gender Classifier Models}
\begin{tabular}{l l | c c r }
\multicolumn{2}{l}{\textbf{Model 1}} & \multicolumn{2}{c}{Predicted} & Recall \\
& & F & M &  \\ 
\cline{2-5}
Actual & F & 590 &  10 & 0.9833 \\
& M & 12 & 572 & 0.9795 \\  \hline
Precision&  & 0.9801 & 0.9828 \\ 
Accuracy & & &  & 0.9814 \\
\end{tabular}
\begin{tabular}{l l | c c r }
\multicolumn{2}{l}{\textbf{Model 2}} & \multicolumn{2}{c}{Predicted} & Recall \\
& & F& M &  \\ 
\cline{2-5}
Actual & F & 576 &  24 & 0.9650 \\
& M & 5 & 579 & 0.9914 \\  \hline
Precision&  & 0.9914 & 0.9602 \\ 
Accuracy & & &  & 0.9755 \\
\end{tabular}
\label{tab:GenConfusion}
\end{center}
\end{table} 


\section{Language Classifier}
\subsection{Model Architecture}

Three models were trained and evaluated for the language classifier. The structure of each model is shown in Table \ref{tab:LangModels}.  Model 1 has one hidden layer with 12 nodes, Model 2 has one hidden layer with 128 nodes, and Model 3 has two hidden layers with 128 and 64 nodes, respectively.

\begin{table}[!h]
\begin{center}
\caption{Language Classifier Model Architectures. }
\begin{tabular}{l | c |c  | c |}

Layer  & Model 1 & Model 2 & Model 3\\
\hline

Input 	& (32, 10, 128)& (32, 10, 128) & (32, 10, 128) \\ \hline

Dense	& 12 nodes 	& 128 nodes 	& 128 nodes \\
Dropout	& 50\%		& 50\%		& 50 \% \\ \hline

Dense	&			&			& 64 nodes \\
Dropout	&			& 			& 50\% \\ \hline

Flatten 	& (32, 120)	& (32, 1280)	& (32, 640) \\ \hline
Output 	& (32, 11)		& (32, 11)		& (32, 11)\\
\hline
\end{tabular}

\label{tab:LangModels}
\end{center}
\end{table}

\subsection{Model Performance Comparison}

A naive classifier that always predicted the majority class (Arabic) would have an accuracy of 15\%.  All three of the models improved upon this baseline accuracy rate, as shown in the second row of Table \ref{tab:LangMetricsSum}. 

\begin{table}[h]
\begin{center}
\caption{Language Classifiers - Metrics Comparison}
\begin{tabular}{l c c c}
		&Model  1		& Model 2		&Model 3 \\ \hline
Loss		&2.25485		&2.323		&2.20684 \\
Accuracy	&0.232955	&0.238636	&0.25\\
Precision	&0.625		&0.5			&0.605263\\
Recall	&0.0142045	&0.0738636	&0.0653 \\ 

F1 Macro	&0.16999		&0.195512	&0.191103\\
F1 Weighted	&0.20067	&0.228178	&0.224565\\
\end{tabular}
\label{tab:LangMetricsSum} 
\end{center}
\end{table} 


\subsection{Confusion Matrix - Model 3}

The confusion matrix for the Model 3 predictions is shown in Table \ref{tab:LangConfMat}, with the precision and recall rates by class in Table \ref{tab:LangClassReport}. 

\begin{table}
\begin{center}
\caption{Confusion matrix for Model 3 predictions. The bold numbers on the diagonal represent correct predictions.}
\begin{tabular}{l | c c c c c c c c c c c || c}
lang			&R &A &T &K &G &D &S &F &E &P &M & Segments\\ \hline
Russian		&\textbf{0}  &2  &1  &4  &0  &1  &7  &4  &1  &1  &7 &28\\
Arabic		&0 &\textbf{24}  &0  &4  &1  &7  &2  &2  &4  &0 &11 &55\\
Turkish		&0  &1  &\textbf{1}  &4  &0  &3  &4  &0   &2  &1  &5 &21\\
Korean		&1 &10  &0  &\textbf{6}  &0  &3  &2  &2  &4  &2  &1 &31\\
German		&0  &0  &0  &1  &\textbf{0}  &0  &5  &1  &5  &2  &1 &15\\
Dutch		&0  &4  &0  &0  &0 &\textbf{12} & 3  &1  &3  &1  &1 &25\\
Spanish		&1  &2  &0  &6  &2  &1 &\textbf{12}  &3  &8  &0  &7 &42\\
French		&1  &3  &0  &4  &1  &2 &10 & \textbf{5}  &1  &1  &4 &32\\
English		&0  &1  &0  &3  &0  &5  &2  &2 &\textbf{13}  &0  &4 &30\\
Portuguese	&0  &3  &0  &3  &1  &0  &7  &5  &3  &\textbf{0}  &6 &28\\
Mandarin		&0  &2 & 0  &8  &2  &4  &9  &1  &2  &2 &\textbf{15} &45\\ \hline
Total			&3 &52 &2 &43 &7 &38 &63 &26 &46 &10 &62 &\textbf{352}\\

\end{tabular}
\label{tab:LangConfMat}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Model 3 Metrics by Language}
\begin{tabular}{l c c c| c}
Language  &Precision &Recall &F1-score&Samples\\ \hline
Russian	&0.00	&0.00	&0.00	&28\\
Arabic	&0.46	&0.44	&0.45	&55\\     
Turkish	&0.50	&0.05	&0.09	&21\\      
Korean	&0.14	&0.19 	&0.16	&31\\      
German	&0.00	&0.00	& 0.00 	&15\\       
Dutch	&0.32	&0.48	&0.38	&25\\   
Spanish	&0.19	&0.29	&0.23	&42\\      
French	&0.19	&0.16	&0.17	&32\\    
English	&0.28	&0.43	&0.34	&30\\  
Portuguese&0.00	&0.00	&0.00	&28\\    
Mandarin	&0.24	&0.33	&0.28	&45\\ \hline
      
accuracy		&		&		&0.25	&352\\
macro avg		& 0.21	&0.22	&0.19	&352\\
weighted avg	& 0.23	&0.25	&0.22	&352\\ \hline
\end{tabular}
\label{tab:LangClassReport}
\end{center}
\end{table}

\end{document}
% - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - -  - - - - 
Groupby language, sum
Sorted in descending order by \% segments

L	Language	Seg	Sp	\%_sp		\%_seg		\%_sp_data	\%_seg_data
A	Arabic	380	75	12.077295	15.109344	36.129857	46.537869
S	Spanish	306	75	12.077295	12.166998	36.129857	35.816711
M	Mandarin	298	65	10.466989	11.848907		31.420528	36.227442
F	French	247	63	10.144928	9.821074		30.498599	28.884452
E	English	239	75	12.077295	9.502982		36.129857	28.463836
K	Korean	237	52	8.373591		9.423459		25.156360	27.267382
R	Russian	195	48	7.729469		7.753479		23.090713	23.331666
P	Portuguese	184	48	7.729469		7.316103		23.090713	21.695100
D	Dutch	164	47	7.568438		6.520875		22.801695	20.038110
T	Turkish	140	37	5.958132		5.566600		18.092367	16.974590
G	German	125	36	5.797101		4.970179		17.459455	14.762841

\begin{table}
\begin{center}
\caption{Confusion matrix for Model 2 predictions.The bold numbers on the diagonal represent correct predictions.}
\begin{tabular}{l | c c c c c c c c c c c || c}
Language			&R &A &T &K &G &D &S &F &E &P &M & Segments\\ \hline

Russian	& \textbf{1}	&2	& 2	& 0	& 3	& 0	& 4	& 4	& 1	& 3	& 8	& 28\\
Arabic	& 0	& \textbf{26}	&  0	&  0	&  2	&  6	&  4	&  0	& 3	& 1	& 13	& 55\\
Turkish	& 1	&  2	&  \textbf{2}	&  2	&  1	&  4	&  4	&  1	& 1	& 1	&  2	& 21\\
Korean	& 5	&  4 	& 2	& \textbf{4}	&  0	&  3	&  4	&  1	& 1	& 3	&  4	& 31\\
German	& 1	&  0	&  0	&  0	& \textbf{0}	&  1	&  3	&  2	& 4	& 3	&  1	& 15\\
Dutch	& 1	&  1	& 1	&  0	&  1	& \textbf{12}	& 1	&  1	& 3	 & 2	&  2	& 25\\
Spanish	& 3	&  1	&  3	&  0	&  4	&  2	&  \textbf{6}	&  2	& 10	& 2	&  9	& 42\\
French	& 2	&  4	&  1	&  2	&  1	&  1	&  9	&  \textbf{3}	& 3	& 2	&  4	& 32\\
English	& 1	&  1	&  0	&  1	&  0	&  3	 & 2	&  0	& \textbf{15}	& 0	&  7	& 30\\
Portuguese	& 7	&  3	&  1	&  3	&  1	&  1	&  1	&  2	&  4 &  \textbf{1}	&  4	& 28\\
Mandarin	& 1	&  0	&  3	&  5	&  1	&  5	&  9	&  3	&  3	&  1	& \textbf{14}	& 45\\  \hline
Total Predictions	&23	& 44 & 15 &17 & 14 & 38  & 47 & 19 & 48 & 19 & 68 & 352 \\
\end{tabular}
\label{tab:LangConfMatMod2}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Metrics (precision, recall and F1 score) by language class - Models 2\& 3}
\begin{tabular}{l c c c| c c c| c}
& \multicolumn{3}{c}{Model 2} & \multicolumn{3}{c}{Model 3} & \\
Language  &precision &recall	&f1-score	&precision &recall &f1-score&support \\ \hline
Russian	&0.04	&0.04      	&0.04	&0.00	&0.00	&0.00	&28\\
Arabic	&0.59	&0.47      	&0.53	&0.46	&0.44	&0.45	&55\\     
Turkish	&0.13	&0.10      	&0.11	&0.50	&0.05	&0.09	&21\\      
Korean	&0.24	&0.13      	&0.17	&0.14	&0.19 	&0.16	&31\\      
German	&0.00	&0.00	&0.00	&0.00	&0.00	& 0.00 	&15\\       
Dutch	&0.32     	&0.48      	&0.38	&0.32	&0.48	&0.38	&25\\   
Spanish	&0.13     	&0.14      	&0.13	&0.19	&0.29	&0.23	&42\\      
French	&0.16      	&0.09      	&0.12	&0.19	&0.16	&0.17	&32\\    
English	&0.31      	&0.50      	&0.38	&0.28	&0.43	&0.34	&30\\  
Portuguese&0.05      &0.04      	&0.04	&0.00	&0.00	&0.00	&28\\    
Mandarin	&0.21	&0.31      	&0.25	&0.24	&0.33	&0.28	&45\\ \hline
      
accuracy		&		&		&0.24	&		&		&0.25	&352\\
macro avg		&0.20	&0.21	&0.20	& 0.21	&0.22	&0.19	&352\\
weighted avg	& 0.23	&0.24	&0.23	& 0.23	&0.25	&0.22	&352\\ \hline
\end{tabular}
\label{tab:LangClassReport23}
\end{center}
\end{table}

